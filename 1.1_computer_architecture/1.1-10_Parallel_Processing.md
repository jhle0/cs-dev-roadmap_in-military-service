# 1.1-10 Parallel Processing

> Active Recall
> 
> - Why is parallel processing needed?
> - What are Flynnâ€™s four classifications, and which one is rarely used in practice?
> - What is the difference between SIMD and MIMD?
> - What are the strengths of GPUs?
> - In what situations are GPUs less efficient than CPUs?
> - What do Amdahlâ€™s Law and Gustafsonâ€™s Law mean?

---

## 1. Concept of Parallel Processing

- **Definition**: Dividing a task into multiple parts that are executed simultaneously by multiple processors or cores.
- **Purpose**: Improve performance, reduce execution time, and handle large-scale data.
- Modern computers cannot endlessly increase single-core speed (due to heat and power limits) â†’ rely on **multicore CPUs, GPUs, and parallel architectures**.

---

## 2. Flynnâ€™s Taxonomy

Parallel architectures are categorized into four types:

| Category | Instructions | Data | Examples |
| --- | --- | --- | --- |
| **SISD** | Single | Single | Traditional single-core CPU |
| **SIMD** | Single | Multiple | Vector processors, some GPU operations |
| **MISD** | Multiple | Single | Rare (e.g., fault-tolerant pipelines) |
| **MIMD** | Multiple | Multiple | Multicore CPUs, cluster servers |

ðŸ‘‰ **MISD** exists mainly as a theoretical concept and is rarely implemented in real systems.

---

## 3. SIMD (Single Instruction, Multiple Data)

- **Concept**: A single instruction operates on multiple data items at the same time.
- **Use case**: Works best for large datasets requiring the same operation.
- **Examples**:
    - Vector/matrix operations (image filtering, pixel processing)
    - Multimedia instructions (Intel SSE, AVX)
- **Pros**: Exploits data parallelism â†’ very fast for uniform tasks.
- **Cons**: Inefficient for tasks with frequent branching or irregular control flow.

---

## 4. MIMD (Multiple Instruction, Multiple Data)

- **Concept**: Multiple instructions operate on multiple data items concurrently.
- **Use case**: Each processor can run different programs independently.
- **Examples**:
    - Multicore CPUs (each core runs different threads)
    - Clusters and distributed systems (each node runs its own task)
- **Pros**: Very flexible; used in most modern architectures.
- **Cons**: Synchronization and shared resource management are complex.

---

## 5. SIMD vs MIMD

| Aspect | SIMD | MIMD |
| --- | --- | --- |
| Instructions | Single | Multiple |
| Data | Multiple | Multiple |
| Best suited for | Repeated identical operations (vectors, matrices) | Independent parallel tasks |
| Examples | Vector processors, GPU operations | Multicore CPUs, cluster servers |

---

## 6. GPU (Graphics Processing Unit)

- **Concept**: Originally designed for graphics (pixel, vector, matrix) acceleration.
    
    Now evolved into **massively parallel processors with thousands of cores**, essential for AI and HPC.
    
- **Strengths**:
    1. **Massive parallelism**: Thousands of threads in parallel
    2. **Optimized for data parallel workloads**: Deep learning, scientific computing, simulations
    3. **Higher FLOPS throughput** than CPUs
- **CPU vs GPU**:

| Aspect | CPU | GPU |
| --- | --- | --- |
| Design goal | General-purpose computing | Large-scale parallelism |
| Core count | Few to dozens | Thousands |
| Single-thread performance | Strong | Weak |
| Data-parallel efficiency | Poor | Excellent |
- **Limitations**:
    - Poor at tasks with heavy branching or complex control flow
    - High power consumption
    - Less efficient than CPUs for general-purpose tasks

---

## 7. Performance Optimization Laws

### 7.1 Amdahlâ€™s Law

- **Definition**: Since only part of a program can be parallelized, overall speedup has an upper limit.
- **Formula**:
    
    $Speedup = \frac{1}{(1-P) + \frac{P}{N}}$
    
    - P: fraction of the program that can be parallelized
    - N: number of processors
- **Key idea**: The non-parallel portion becomes the bottleneck.
- **Example**: Even if 90% of a program is parallelizable, the remaining 10% serial portion limits maximum speedup.

---

### 7.2 Gustafsonâ€™s Law

- **Definition**: As problem size grows, the benefit of parallelization increases.
- **Formula**:
    
    $Speedup = N - (1-P) \times (N-1)$
    
    - P: fraction of the program that can be parallelized
    - N: number of processors
- **Key idea**: In practice, workloads scale with data size, so parallel architectures deliver greater gains.
- **Example**: The larger the AI dataset (tens of GBs), the more effective GPU parallelization becomes.

---

## 8. Real-world Applications

- **SIMD**: Image filters, multimedia codecs, signal processing, cryptography acceleration
- **MIMD**: Web server multithreading, database transactions, cloud/distributed systems, simulations
- **GPU**: Deep learning training/inference, autonomous driving sensor processing, scientific simulations, financial modeling
- **Performance Laws in Practice**:
    - *Amdahlâ€™s Law*: Minimize serial code to maximize parallel efficiency
    - *Gustafsonâ€™s Law*: Exploit larger datasets to scale performance with more processors
