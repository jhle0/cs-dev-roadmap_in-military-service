# 1.1-8 Memory Hierarchy

> Active recall
> 
> - Why do we need a memory hierarchy?
> - What is the order of the memory hierarchy?
> - What is locality?
> - What are the 3 types of cache misses?
> - What are the 3 cache mapping techniques? (with address breakdown)
> - What are the cache replacement policies and real-world usage?
> - What are the cache write policies? When is a dirty bit needed?
> - How can developers improve cache efficiency?

---

## Why do we need a memory hierarchy?

CPU speed has increased rapidly every year, but **memory speed (especially RAM and disk) has not kept up.**

→ This mismatch causes the **Memory Wall problem**, where the CPU is stalled waiting for data.

**Solution: a hierarchy structure.**

- **Closer to the CPU (registers, cache)** → faster, smaller, more expensive
- **Farther away (RAM, SSD/HDD)** → slower, larger, cheaper
- Idea: **Keep frequently used data in fast memory, less-used data in slower memory.**

---

## Structure of the Memory Hierarchy

1. **Registers**
    - Inside the CPU, the fastest storage
    - Very small (a few dozen to a few hundred)
    - Hold operands, addresses, and control values
2. **Cache Memory**
    - Between CPU and RAM, ultra-fast (MBs in size)
    - **L1, L2, L3 caches**
        - L1/L2: private per core
        - L3: shared across cores
    - Purpose: reduce RAM access and keep CPU busy
3. **Main Memory (RAM, DRAM)**
    - Stores active programs and data
    - Tens of times slower than cache
4. **Secondary Storage (SSD/HDD)**
    - Stores programs, files, databases long-term
    - Very large, but very slow

---

## Locality Principle

Cache works because of **locality**: the tendency of programs to access the same or nearby data.

- **Temporal locality**: recently accessed data will likely be reused soon
    - e.g., loop variable `i` in a for-loop
- **Spatial locality**: data near recently accessed data will likely be used soon
    - e.g., accessing `arr[0], arr[1], arr[2]` in sequence

---

## How Cache Works

- Cache loads data from RAM in **blocks (32–128B)** → exploiting spatial locality
- **Cache hit**: requested data is already in cache → fast
- **Cache miss**: not in cache → must fetch from RAM → slow

### Types of Cache Misses

1. **Cold (Compulsory) miss**: first-time access, must load from RAM
2. **Capacity miss**: cache is too small, data is evicted
3. **Conflict miss**: multiple blocks map to the same cache location

---

## Cache Mapping Techniques

Rules that decide **where a memory block is stored in cache**.

1. **Direct Mapping**
    - Each memory block maps to exactly one cache line
    - Address breakdown: `[Tag | Index | Block offset]`
    - ✅ Simple and fast
    - ❌ High conflict misses
2. **Fully Associative Mapping**
    - A memory block can go **anywhere in cache**
    - Address breakdown: `[Tag | Block offset]`
    - ✅ Low conflict misses
    - ❌ Expensive hardware (must search all cache lines)
3. **Set-Associative Mapping**
    - Cache is divided into sets; a block can go anywhere within its set
    - Address breakdown: `[Tag | Set index | Block offset]`
    - ✅ Balance of performance and cost
    - → Most common in modern CPUs

---

## Cache Replacement Policies

When cache is full, decide **which block to evict**.

- **LRU (Least Recently Used)**
    - Evict the block unused for the longest time
    - Best fit for locality, but costly to implement in hardware
    - Used in CPU caches and OS page replacement algorithms
- **FIFO (First-In First-Out)**
    - Evict the oldest block
    - Simple but may remove frequently used data
- **Random**
    - Evict a random block
    - Simple hardware, less effective than LRU

---

## Cache Write Policies

When the CPU writes data to cache, how to update RAM?

1. **Write-through**
    - Update both cache and RAM immediately
    - ✅ Strong consistency
    - ❌ Slower writes
2. **Write-back**
    - Update cache only, and write to RAM **when the block is evicted**
    - ✅ Faster writes
    - ❌ Requires extra management → **Dirty bit**
        - Dirty bit = a flag showing the cache block has been modified

---

## Performance Optimization (Developer’s Perspective)

- **Loop optimization**: match memory access order to storage layout
- **Data structures**: arrays (contiguous) > linked lists (scattered)
- **Avoid false sharing**: prevent multiple threads from writing to the same cache line

---

## Real-world Applications

- **Databases (DBMS)**: B+ tree indexes minimize disk access using locality
- **GPU computing**: memory coalescing to maximize spatial locality
- **Operating systems**: page replacement algorithms (e.g., LRU) are based on the same principles
