# 1.1-8 Memory Hierarchy

> **Active Recall**
> 
> - Why is a memory hierarchy needed?
> - What is the order of the memory hierarchy?
> - What is locality?
> - What are the three types of cache misses?
> - How can developers improve cache efficiency?

## Why Do We Need a Memory Hierarchy?

CPU performance has improved dramatically over the years, but **memory (especially RAM and disk) has not kept up in speed**.

This means the CPU is fast, but fetching data is relatively slow, creating a bottleneck → this is called the **Memory Wall**.

The solution is to build a **hierarchical structure**:

- **Closer to the CPU (registers, cache)** → very fast, very expensive, but small in capacity.
- **Farther away (RAM, SSD/HDD)** → slower, cheaper, but large in capacity.

→ Key idea: **Keep frequently used data in the faster layers, and rarely used data in the slower layers.**

---

## Memory Hierarchy Structure

1. **Register**
    - The fastest storage, located inside the CPU.
    - Size: only dozens to hundreds in count (not even KB).
    - Stores operands, addresses, and values needed for instruction execution.
2. **Cache Memory**
    - Ultra-fast memory between the CPU and RAM (a few MB to tens of MB).
    - Divided into **L1, L2, and L3 caches** (L1 is fastest and smallest).
    - L1/L2 caches are private to each core, while L3 is shared among multiple cores.
    - Purpose: reduce RAM access so the CPU doesn’t stay idle.
3. **Main Memory (RAM, DRAM)**
    - Stores programs and data currently in use.
    - Tens of times slower than cache.
4. **Secondary Storage (SSD/HDD)**
    - Stores programs, files, and databases long-term.
    - Very large capacity but extremely slow compared to RAM.

---

## Principle of Locality

Cache works by predicting **what the CPU is likely to use next**.

This prediction is effective because of **locality**.

- **Temporal Locality**
    
    → “Data that was recently used is likely to be used again soon.”
    
    Example: repeatedly accessing the variable `i` inside a loop.
    
- **Spatial Locality**
    
    → “If a certain data item is used, nearby data will likely be used soon as well.”
    
    Example: accessing `arr[0]`, `arr[1]`, `arr[2]` sequentially in an array.
    

---

## How Cache Works

Cache loads data from RAM in **blocks** (usually 32B ~ 128B).

→ This block-based design leverages **spatial locality**.

- **Cache Hit**: the required data is already in the cache (fast).
- **Cache Miss**: the required data is not in the cache, so it must be fetched from RAM (slow).

**Types of Cache Misses**

1. **Cold (Compulsory) Miss**: first-time access → must load from RAM.
2. **Capacity Miss**: the cache is too small, so old data is evicted.
3. **Conflict Miss**: data mapping policy (e.g., direct-mapped cache) causes collisions.

---

## Performance Optimization Techniques

From a developer’s perspective, **improving locality in code increases cache efficiency.**

- **Loop Optimization**
    
    ```cpp
    // Inefficient: stored in row-major order but accessed by column
    for (int j = 0; j < N; j++) {
        for (int i = 0; i < N; i++) {
            sum += A[i][j];
        }
    }
    
    // Efficient: access memory in the stored order (↑ locality)
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            sum += A[i][j];
        }
    }
    
    ```
    
- **Choosing Data Structures**
    - Array: stored contiguously in memory → good spatial locality.
    - Linked List: nodes scattered in memory → poor cache efficiency.
- **Avoiding False Sharing in Multithreading**
    - Even if threads use different variables, if they fall within the same cache line, conflicts occur → performance degradation.

---

## Real-World Applications

- **DBMS**: Indexes (e.g., B+ trees) are designed to minimize disk access and exploit locality.
- **GPU Computation**: Thousands of threads access memory simultaneously → memory coalescing (contiguous access) is critical.
- **Operating Systems**: Page replacement algorithms (e.g., LRU) are based on the assumption of temporal locality (“recently accessed data will likely be used again”).
