# 1.1-8 메모리 계층

> Active recall
> 
> - 왜 메모리 계층이 필요한가?
> - 메모리 계층의 순서
> - 지역성이란 무엇인가?
> - Cache Miss 종류 3가지는?
> - Cache 매핑 기법 3가지는?
> - Cache 교체 정책 종류는?
> - Cache 쓰기 정책에는 무엇이 있는가?
> - 개발자가 캐시 효율을 높이는 방법은?

## 왜 메모리 계층이 필요한가?

CPU 속도는 매년 비약적으로 빨라졌지만, **메모리(특히 RAM, 디스크)의 속도는 상대적으로 느리다.**

즉, CPU는 빠른데 데이터를 가져오는 게 느려서 발목 잡히는 문제가 생긴다 → 이를 **Memory Wall**이라고 부른다.

그래서 해결책으로 **계층형 구조(Hierarchy)**를 만든 것이다:

- **가까운 곳(레지스터, 캐시)** → 빠르고 비싸며 용량이 적음
- **먼 곳(RAM, SSD/HDD)** → 느리고 싸며 용량이 큼

→ 아이디어: **자주 쓰는 데이터는 빠른 계층에 두고, 안 쓰는 데이터는 느린 계층에 둔다.**

---

## 메모리 계층 구조

1. **레지스터 (Register)**
    - CPU 내부에 있는 가장 빠른 저장소.
    - 크기는 몇십~몇백 개 수준 (KB 단위도 안 됨).
    - 명령어 실행에 필요한 피연산자, 주소 등을 저장.
2. **캐시 메모리 (Cache)**
    - CPU와 RAM 사이에 있는 초고속 메모리. (수 MB ~ 수십 MB)
    - **L1, L2, L3 캐시**로 나뉨 (L1이 가장 빠르고 작음).
    - L1/L2 캐시는 코어마다 따로 있고, L3는 여러 코어가 공유한다.
    - 목적: RAM 접근을 줄여 CPU가 놀지 않도록 함.
3. **주기억장치 (RAM, DRAM)**
    - 실행 중인 프로그램과 데이터를 저장.
    - 캐시보다 수십 배 느림.
4. **보조기억장치 (SSD/HDD)**
    - 프로그램, 파일, 데이터베이스를 장기 저장.
    - 용량은 크지만 매우 느림.

---

## 지역성(Locality) 원리

캐시 메모리는 지역성 원리에 의해 **CPU가 사용할 법한 대상**을 **예측**하여 저장한다
이 예측이 잘 맞는 이유는 **지역성(Locality)** 때문이다.

- **시간 지역성 (Temporal Locality)**
    
    → "최근에 사용한 데이터는 곧 다시 사용할 가능성이 높다."
    
    예: 반복문 안에서 `i` 변수 계속 접근.
    
- **공간 지역성 (Spatial Locality)**
    
    → "지금 사용한 데이터 근처의 데이터도 곧 사용할 가능성이 높다."
    
    예: 배열 `arr[0]`, `arr[1]`, `arr[2]` 순서대로 접근.
    

---

## 캐시 동작 원리

캐시는 RAM 내용을 "블록(Block)" 단위로 가져온다. (보통 32B ~ 128B)
     → 캐시가 블록 단위로 가져오는 이유는 **공간 지역성** 때문이다.

- **캐시 적중(Cache Hit)**: CPU가 필요한 데이터가 캐시에 이미 있는 경우 (빠르게 처리됨).
- **캐시 실패(Cache Miss)**: 캐시에 없어서 RAM에서 다시 가져와야 하는 경우 (느려짐).

캐시 미스는 3가지로 나뉜다:

1. **Cold miss (Compulsory)**: 처음 접근하는 데이터 → 무조건 RAM에서 불러와야 함.
2. **Capacity miss**: 캐시 용량이 부족해 기존 데이터가 밀려난 경우.
3. **Conflict miss**: 캐시 매핑 정책(예: Direct mapped) 때문에 다른 데이터랑 충돌.

---

## 캐시 매핑 기법

메모리 블록이 캐시의 어디에 저장될지 결정하는 규칙.

1. **Direct Mapping (직접 매핑)**
    - 하나의 메모리 블록 → 캐시 한 위치에만 저장 가능
    - 주소 분해: `[Tag | Index | Block offset]`
    - 장점: 단순·빠름
    - 단점: 충돌 미스 ↑
2. **Fully Associative Mapping (완전 연관 매핑)**
    - 메모리 블록이 캐시 어느 곳에도 저장 가능
    - 주소 분해: `[Tag | Block offset]` (Index 없음)
    - 장점: 충돌 ↓
    - 단점: 탐색 비용 큼, 구현 복잡
3. **Set-Associative Mapping (집합 연관 매핑)**
    - 캐시를 여러 Set으로 나누고, 각 Set 안에서는 자유롭게 저장 가능
    - 주소 분해: `[Tag | Set index | Block offset]`
    - 장점: Direct보다 충돌 ↓, Fully보다 효율 ↑
    - 실제 CPU에서 가장 널리 쓰임

---

## 캐시 교체 정책

캐시가 꽉 찼을 때 어떤 블록을 내보낼지 결정.

- **LRU (Least Recently Used)**: 가장 오래 안 쓰인 데이터 제거
    - 지역성 가정에 가장 적합, 하드웨어 구현은 복잡
    - CPU 캐시, OS 페이지 교체 알고리즘의 기본 아이디어
- **FIFO (First-In First-Out)**: 가장 먼저 들어온 데이터 제거
    - 구현 단순하지만, 최근 쓰이는 데이터가 밀려날 수 있음
- **Random**: 무작위로 제거
    - 구현 단순, 성능은 LRU보다 떨어짐

---

## 캐시 쓰기 정책

CPU가 캐시에 데이터를 쓸 때, RAM과 동기화하는 방식.

1. **Write-through**
    - 캐시에 쓰는 동시에 RAM에도 반영
    - 장점: 데이터 일관성 유지 ↑
    - 단점: 쓰기 속도 ↓
2. **Write-back**
    - 캐시에만 먼저 쓰고, 해당 블록이 교체될 때 RAM에 반영
    - 장점: 쓰기 성능 ↑
    - 단점: 일관성 관리 복잡 → **Dirty bit** 필요
        - Dirty bit = 해당 블록이 캐시에서 수정되었는지 표시하는 플래그

---

## 성능 최적화 방식 (개발자 관점)

- **루프 최적화**: 메모리 접근 순서 → 행 단위 접근으로 캐시 효율 ↑.
- **데이터 구조**: 배열(연속적) vs 연결리스트(산발적).
- **False Sharing 방지**: 스레드 간 캐시 라인 충돌 최소화.

---

## 실무 활용 예시

- **DBMS**: 인덱스를 B+트리로 설계 → 디스크 접근 최소화 & 지역성 활용.
- **GPU 연산**: 메모리 coalescing으로 공간 지역성 극대화.
- **운영체제**: 페이지 교체 알고리즘(LRU 등)도 캐시 원리와 유사.
